{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "“Lab3 Subword-tokenizations”的副本",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fyfy1996/Natural_language_understanding/blob/master/Lab3_Subword_tokenizations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKyn4Uuk2_gt",
        "colab_type": "text"
      },
      "source": [
        "# Subword Tokenization\n",
        "\n",
        "We will use a subword tokenization library from Huggingface.\n",
        "Let's install the tokenizer and download the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_xCE5TC0ljj",
        "colab_type": "code",
        "outputId": "0c25140f-0c99-4d22-e00e-6b31ec3ff904",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        }
      },
      "source": [
        "!pip install tokenizers\n",
        "# download some sample text \n",
        "!wget https://raw.githubusercontent.com/google/sentencepiece/master/data/botchan.txt\n",
        "!wget https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tokenizers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2e/0a/096a279ee20a206a58ba9af3251c7271fb9493bed3bf29bfe4c58fcbfe4c/tokenizers-0.4.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 2.9MB/s \n",
            "\u001b[?25hInstalling collected packages: tokenizers\n",
            "Successfully installed tokenizers-0.4.2\n",
            "--2020-02-13 16:22:40--  https://raw.githubusercontent.com/google/sentencepiece/master/data/botchan.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 278779 (272K) [text/plain]\n",
            "Saving to: ‘botchan.txt’\n",
            "\n",
            "botchan.txt         100%[===================>] 272.25K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2020-02-13 16:22:40 (5.71 MB/s) - ‘botchan.txt’ saved [278779/278779]\n",
            "\n",
            "--2020-02-13 16:22:41--  https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.39.70\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.39.70|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 213450 (208K) [text/plain]\n",
            "Saving to: ‘bert-base-cased-vocab.txt’\n",
            "\n",
            "bert-base-cased-voc 100%[===================>] 208.45K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2020-02-13 16:22:41 (5.43 MB/s) - ‘bert-base-cased-vocab.txt’ saved [213450/213450]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbEWzm3cipRr",
        "colab_type": "text"
      },
      "source": [
        "# Import Tokenizer Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxO4woDBPZUO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tokenizers import (ByteLevelBPETokenizer,\n",
        "                            SentencePieceBPETokenizer,\n",
        "                            BertWordPieceTokenizer)\n",
        "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4bFr_m7dRad",
        "colab_type": "text"
      },
      "source": [
        "# First let's try out a pretrained WordPiece tokenizer used in BERT model.\n",
        "\n",
        "You will notice the [CLS] and [SEP] tokens which are usually added at the beginning and end of a sentence in BERT model. You will learn more about BERT in lecture.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4fV-65_dYXp",
        "colab_type": "code",
        "outputId": "b525a7d8-2030-485a-8087-53caa0f828c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Intialize tokenizer with BERT's BPE vocabulary\n",
        "bert_tokenizer = BertWordPieceTokenizer(\"bert-base-cased-vocab.txt\")\n",
        "\n",
        "# Tokenize text\n",
        "output = bert_tokenizer.encode(\"Hellooooooooo! How are you?\")\n",
        "print(output.tokens)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['[CLS]', 'hello', '##oo', '##oo', '##oo', '##oo', '!', 'how', 'are', 'you', '?', '[SEP]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBQUJ4jFerk6",
        "colab_type": "text"
      },
      "source": [
        "# Now, let's train your own custom WordPiece tokenizer.\n",
        "We downloaded a file named `botchan.txt` at the beginning. Let's train a wordpiece model on this file.  \n",
        "\n",
        "Recap: WordPiece expects the pretokenized word sequence.  \n",
        "The `tokenizer` library has built-in pretokenizer for WordPiece model so you don't have to worry about it. But, if you want to, you can specify the type of builtin pretokenizer you want to use. \n",
        "\n",
        "You can check the list of built-in pretokenizers [here](https://github.com/huggingface/tokenizers/blob/master/bindings/python/src/pre_tokenizers.rs)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cKKrg9ffZlw",
        "colab_type": "code",
        "outputId": "403796b1-907e-455e-9615-8703dde5af3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# First, initialize a new WordPiece tokenizer\n",
        "my_tokenizer = BertWordPieceTokenizer()\n",
        "\n",
        "# Customize pre-tokenization and decoding\n",
        "my_tokenizer.pre_tokenizer = pre_tokenizers.WhitespaceSplit(add_prefix_space=True)\n",
        "my_tokenizer.decoder = decoders.WordPiece()\n",
        "\n",
        "# And then train on some text file\n",
        "my_tokenizer.train([\"botchan.txt\"])\n",
        "\n",
        "# Now we can encode\n",
        "my_tok_output = my_tokenizer.encode(\"Hellooooooooo! How are you?\")\n",
        "print(\"tokens: \", my_tok_output.tokens)\n",
        "print(\"offsets:\", my_tok_output.offsets)\n",
        "print(\"getting a token from offsets:\", my_tok_output.original_str[my_tok_output.offsets[0]])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tokens:  ['hello', '##oo', '##oo', '##oo', '##oo', '!', 'how', 'are', 'you', '?']\n",
            "offsets: [(0, 5), (5, 7), (7, 9), (9, 11), (11, 13), (13, 14), (15, 18), (19, 22), (23, 26), (26, 27)]\n",
            "getting a token from offsets: Hello\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvMNp2QifoqP",
        "colab_type": "text"
      },
      "source": [
        "You can see that the output is different from pretrained Bert's WordPiece tokenizer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekDr7a0duz-s",
        "colab_type": "text"
      },
      "source": [
        "# Now, let's train a SentencePiece Tokenizer.\n",
        "\n",
        "SentencePieceBPETokenizer is already implemented for you so you don't have to define trainer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXZsSWBwWulR",
        "colab_type": "code",
        "outputId": "0b83a379-c287-4b5d-8c8b-295a06f366bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# First, initialize a new SentencePiece tokenizer\n",
        "my_sp_tokenizer = SentencePieceBPETokenizer()\n",
        "\n",
        "# And then train\n",
        "my_sp_tokenizer.train([\"botchan.txt\"], vocab_size=20000, min_frequency=2)\n",
        "\n",
        "# Now we can encode\n",
        "my_sp_tok_output = my_sp_tokenizer.encode(\"Hellooooooooo! How are you?\")\n",
        "print(my_sp_tok_output.tokens)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['▁Hell', 'oo', 'oo', 'oo', 'oo', 'o', '!', '▁How', '▁are', '▁you?']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEmuqhG0vsV2",
        "colab_type": "text"
      },
      "source": [
        "# Similary, let's train a Byte-Level Tokenizer.\n",
        "\n",
        "`Tokenizer`'s BBPE adds a special token 'Ġ' at the beginning of words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Owi-ZJipkJdM",
        "colab_type": "code",
        "outputId": "2154d348-47b5-46f8-9543-365034a37d69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# First, initialize a new Byte-level tokenizer\n",
        "my_bbpe_tokenizer = ByteLevelBPETokenizer()\n",
        "\n",
        "# And then train\n",
        "my_bbpe_tokenizer.train([\"botchan.txt\"], vocab_size=20000, min_frequency=2)\n",
        "\n",
        "# Now we can encode\n",
        "my_bbpe_tok_output = my_bbpe_tokenizer.encode(\"Hellooooooooo! How are you?\")\n",
        "print(my_bbpe_tok_output.tokens)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Hell', 'oo', 'oo', 'oo', 'oo', 'o', '!', 'ĠHow', 'Ġare', 'Ġyou', '?']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qp9X3XRo9a-C",
        "colab_type": "text"
      },
      "source": [
        "## How can we get back the tokens before BPE?\n",
        "\n",
        "### For word piece:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNnsjSKw8ikx",
        "colab_type": "code",
        "outputId": "6328bd46-b25e-4166-cfa5-34a0301e9ae3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "\"\"\"\n",
        "TODO: Your code here\n",
        "\"\"\"\n",
        "my_tokenizer.decode(my_tok_output.ids).replace(\" ##\",\"\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'hellooooooooo ! how are you ?'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJdaAIKV984f",
        "colab_type": "text"
      },
      "source": [
        "For SentencePiece:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrTUBIjE973l",
        "colab_type": "code",
        "outputId": "a79d0b67-4cd7-488d-8179-cc883ff31e7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "\"\"\"\n",
        "TODO: Your code here\n",
        "\"\"\"\n",
        "my_sp_tokenizer.decode(my_sp_tok_output.ids)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hellooooooooo! How are you?'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pK3n6evYkoNq",
        "colab_type": "text"
      },
      "source": [
        "For byte-level BPE:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOBNixJskb-O",
        "colab_type": "code",
        "outputId": "949cc46e-8c4a-43a6-ebc8-f29a1b3e25bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\"\"\"\n",
        "TODO: Your code here\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokens before BBPE:  Hellooooooooo! How are you?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4yd7AaY-pb0",
        "colab_type": "text"
      },
      "source": [
        "As you can see, it's usually pretty straightforward to de-bpe the bpe tokens. Detokenizing the tokens to revert the pretokenization in WordPiece is a different story."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLTZYFb9xBuu",
        "colab_type": "text"
      },
      "source": [
        "# Implement the original BPE algorithm on your own.\n",
        "\n",
        "### Reference: \n",
        "Sennrich, Rico, Barry Haddow, and Alexandra Birch. \"Neural Machine Translation of Rare Words with Subword Units.\" Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Vol. 1. 2016. http://www.aclweb.org/anthology/P16-1162.\n",
        "\n",
        "Recap: Original BPE expects pretokenized sequence of words as input.  \n",
        "\n",
        "\n",
        "Unlike SentencePiece, it doesn't use a special chracter for space. Instead, it uses an end of word symbol, '</w>' in our case.\n",
        "\n",
        "First, let's create a method that adds '</w>' at the end of each word and convert a word into character tokens with space in between.\n",
        "\n",
        "\"NLP\" -> \"N L P \\</w>\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLqaZfIHN1n7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# toy training data\n",
        "train_sents = [\"what is the higher mountain ?\", \"higher than highest\", \"this is the widest river\", \"newest iphone is cool\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNsPjmkmMo54",
        "colab_type": "code",
        "outputId": "17dc66ac-e865-4ee3-fad4-55ebdfb7acfc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "import collections, re\n",
        "\n",
        "def get_vocab(sentences):\n",
        "    \"\"\"\n",
        "        This function appends </w> at the end of each word\n",
        "        and add space between characters of the word\n",
        "        and count the occurence of each word\n",
        "    \"\"\"\n",
        "    vocab = collections.defaultdict(int)\n",
        "    for sent in sentences:\n",
        "        words = sent.strip().split()\n",
        "        for word in words:\n",
        "            vocab[' '.join(list(word)) + ' </w>'] += 1\n",
        "    return vocab\n",
        "tmp_vocab = get_vocab(train_sents)\n",
        "print(tmp_vocab)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "defaultdict(<class 'int'>, {'w h a t </w>': 1, 'i s </w>': 3, 't h e </w>': 2, 'h i g h e r </w>': 2, 'm o u n t a i n </w>': 1, '? </w>': 1, 't h a n </w>': 1, 'h i g h e s t </w>': 1, 't h i s </w>': 1, 'w i d e s t </w>': 1, 'r i v e r </w>': 1, 'n e w e s t </w>': 1, 'i p h o n e </w>': 1, 'c o o l </w>': 1})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bmWkX1JOdQc",
        "colab_type": "text"
      },
      "source": [
        "We want to merge the most frequently occuring token pairs in our training data.\n",
        "Let's write a function that will return the possible token pairs in our vocabulary and their frequency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5dMaep9NYBn",
        "colab_type": "code",
        "outputId": "4951a832-5f19-4c08-833d-6fb5d72de4fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 809
        }
      },
      "source": [
        "def get_stats(vocab):\n",
        "    \"\"\"\n",
        "       This function return the consecutive token pairs in our data and their count\n",
        "    \"\"\"\n",
        "    pairs = collections.defaultdict(int)\n",
        "    for word, freq in vocab.items():\n",
        "        # add all the consecutive token pairs and their frequencies to counter `pairs`\n",
        "        symbols = word.split()\n",
        "        for i in range(len(symbols)-1):\n",
        "            pairs[symbols[i],symbols[i+1]] += freq\n",
        "    return pairs\n",
        "\n",
        "tmp_pairs = get_stats(tmp_vocab)\n",
        "tmp_pairs"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(int,\n",
              "            {('?', '</w>'): 1,\n",
              "             ('a', 'i'): 1,\n",
              "             ('a', 'n'): 1,\n",
              "             ('a', 't'): 1,\n",
              "             ('c', 'o'): 1,\n",
              "             ('d', 'e'): 1,\n",
              "             ('e', '</w>'): 3,\n",
              "             ('e', 'r'): 3,\n",
              "             ('e', 's'): 3,\n",
              "             ('e', 'w'): 1,\n",
              "             ('g', 'h'): 3,\n",
              "             ('h', 'a'): 2,\n",
              "             ('h', 'e'): 5,\n",
              "             ('h', 'i'): 4,\n",
              "             ('h', 'o'): 1,\n",
              "             ('i', 'd'): 1,\n",
              "             ('i', 'g'): 3,\n",
              "             ('i', 'n'): 1,\n",
              "             ('i', 'p'): 1,\n",
              "             ('i', 's'): 4,\n",
              "             ('i', 'v'): 1,\n",
              "             ('l', '</w>'): 1,\n",
              "             ('m', 'o'): 1,\n",
              "             ('n', '</w>'): 2,\n",
              "             ('n', 'e'): 2,\n",
              "             ('n', 't'): 1,\n",
              "             ('o', 'l'): 1,\n",
              "             ('o', 'n'): 1,\n",
              "             ('o', 'o'): 1,\n",
              "             ('o', 'u'): 1,\n",
              "             ('p', 'h'): 1,\n",
              "             ('r', '</w>'): 3,\n",
              "             ('r', 'i'): 1,\n",
              "             ('s', '</w>'): 4,\n",
              "             ('s', 't'): 3,\n",
              "             ('t', '</w>'): 4,\n",
              "             ('t', 'a'): 1,\n",
              "             ('t', 'h'): 4,\n",
              "             ('u', 'n'): 1,\n",
              "             ('v', 'e'): 1,\n",
              "             ('w', 'e'): 1,\n",
              "             ('w', 'h'): 1,\n",
              "             ('w', 'i'): 1})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KvnZWf9RLsr",
        "colab_type": "text"
      },
      "source": [
        "Now, let's write a function that merges the word pairs in vocabulary.\n",
        "Refer to regex library for pattern matching: https://docs.python.org/3/library/re.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mc80uhwWxBAu",
        "colab_type": "code",
        "outputId": "6d15c79d-ba01-4396-e6fd-b30796affff8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "def merge_vocab(pair, v_in):\n",
        "    v_out = {}\n",
        "    bigram = re.escape(' '.join(pair)) # escape with space\n",
        "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)') # prepare regex query, \n",
        "                                                    # match only if both char in bigram is not already part of a token\n",
        "                                                    # e.g: pair = (e, s), \"l o w e s t\" -> match, \n",
        "                                                    # \"l o we s t\" or \"l o w e st\" -> don't match\n",
        "    for word in v_in:\n",
        "        w_out = p.sub(''.join(pair), word)\n",
        "        v_out[w_out] = v_in[word]\n",
        "    return v_out\n",
        "\n",
        "merge_vocab(('h', 'e'), tmp_vocab)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'? </w>': 1,\n",
              " 'c o o l </w>': 1,\n",
              " 'h i g he r </w>': 2,\n",
              " 'h i g he s t </w>': 1,\n",
              " 'i p h o n e </w>': 1,\n",
              " 'i s </w>': 3,\n",
              " 'm o u n t a i n </w>': 1,\n",
              " 'n e w e s t </w>': 1,\n",
              " 'r i v e r </w>': 1,\n",
              " 't h a n </w>': 1,\n",
              " 't h i s </w>': 1,\n",
              " 't he </w>': 2,\n",
              " 'w h a t </w>': 1,\n",
              " 'w i d e s t </w>': 1}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0XPEitLVHSc",
        "colab_type": "text"
      },
      "source": [
        "Finally, the following function is to keep track of tokens in our vocabulary since the beginning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AlyPDJMR8-q-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_tokens(vocab, tokens=None):\n",
        "    if tokens is None:\n",
        "        tokens = []\n",
        "    for word, freq in vocab.items():\n",
        "        word_tokens = word.split()\n",
        "        tokens.extend(word_tokens)\n",
        "    return list(set(tokens))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zl0sKxDMxhwc",
        "colab_type": "text"
      },
      "source": [
        "# Training BPE\n",
        "\n",
        "Based on the functions above, can you write the training algoritm?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKHRT4Ir9e6A",
        "colab_type": "code",
        "outputId": "8bb96bef-079a-414b-d43c-fe57cd3a8953",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "train_vocab = get_vocab(train_sents)\n",
        "\n",
        "tokens = get_tokens(train_vocab)\n",
        "print(\"vocab before training: \", train_vocab) \n",
        "print(\"number of tokens before training: \", tokens) # All characters"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocab before training:  defaultdict(<class 'int'>, {'w h a t </w>': 1, 'i s </w>': 3, 't h e </w>': 2, 'h i g h e r </w>': 2, 'm o u n t a i n </w>': 1, '? </w>': 1, 't h a n </w>': 1, 'h i g h e s t </w>': 1, 't h i s </w>': 1, 'w i d e s t </w>': 1, 'r i v e r </w>': 1, 'n e w e s t </w>': 1, 'i p h o n e </w>': 1, 'c o o l </w>': 1})\n",
            "number of tokens before training:  ['</w>', 'v', 'p', 't', 'a', 'c', 'u', 'n', 'i', 'e', 's', 'g', 'm', 'd', 'l', 'w', 'h', 'o', '?', 'r']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9zIS7m84eeY",
        "colab_type": "code",
        "outputId": "0b4b8c64-57cc-4298-88b6-1bd8fda75425",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        }
      },
      "source": [
        "num_merges = 5\n",
        "for i in range(num_merges):\n",
        "\n",
        "    # Get the token pair with max frequency\n",
        "    # best_pair = ??\n",
        "    pairs = get_stats(train_vocab)\n",
        "    if not pairs:\n",
        "      break\n",
        "    best_pair = max(pairs, key=pairs.get)\n",
        "    # update train_vocab using the merged best pair \n",
        "    train_vocab = merge_vocab(best, train_vocab)\n",
        "    # update tokens using train_vocab\n",
        "    tokens = get_tokens(train_vocab, tokens)\n",
        "\n",
        "    print('Iter: {}'.format(i))\n",
        "    print('Best pair: {}'.format(best_pair))\n",
        "    print('Tokens: {}'.format(tokens))\n",
        "    print('Number of tokens: {}'.format(len(tokens)))\n",
        "    print('==========')\n",
        "print(\"vocab after training: \", train_vocab)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iter: 0\n",
            "Best pair: ('t', '</w>')\n",
            "Tokens: ['p', 't', 'c', 'u', 's', '</w>', 'v', 'i', 'm', 'l', 'r', 'a', 'd', 'g', 'w', 'h', '?', 'n', 'he', 'e', 'o']\n",
            "Number of tokens: 21\n",
            "==========\n",
            "Iter: 1\n",
            "Best pair: ('t', '</w>')\n",
            "Tokens: ['p', 't', 'c', 'u', 's', '</w>', 'v', 'i', 'm', 'l', 'r', 'a', 'd', 'g', 'w', 'h', '?', 'n', 'he', 'e', 'o']\n",
            "Number of tokens: 21\n",
            "==========\n",
            "Iter: 2\n",
            "Best pair: ('t', '</w>')\n",
            "Tokens: ['p', 't', 'c', 'u', 's', '</w>', 'v', 'i', 'm', 'l', 'r', 'a', 'd', 'g', 'w', 'h', '?', 'n', 'he', 'e', 'o']\n",
            "Number of tokens: 21\n",
            "==========\n",
            "Iter: 3\n",
            "Best pair: ('t', '</w>')\n",
            "Tokens: ['p', 't', 'c', 'u', 's', '</w>', 'v', 'i', 'm', 'l', 'r', 'a', 'd', 'g', 'w', 'h', '?', 'n', 'he', 'e', 'o']\n",
            "Number of tokens: 21\n",
            "==========\n",
            "Iter: 4\n",
            "Best pair: ('t', '</w>')\n",
            "Tokens: ['p', 't', 'c', 'u', 's', '</w>', 'v', 'i', 'm', 'l', 'r', 'a', 'd', 'g', 'w', 'h', '?', 'n', 'he', 'e', 'o']\n",
            "Number of tokens: 21\n",
            "==========\n",
            "vocab after training:  {'w h a t </w>': 1, 'i s </w>': 3, 't he </w>': 2, 'h i g he r </w>': 2, 'm o u n t a i n </w>': 1, '? </w>': 1, 't h a n </w>': 1, 'h i g he s t </w>': 1, 't h i s </w>': 1, 'w i d e s t </w>': 1, 'r i v e r </w>': 1, 'n e w e s t </w>': 1, 'i p h o n e </w>': 1, 'c o o l </w>': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzxZzuImhSyC",
        "colab_type": "text"
      },
      "source": [
        "## Questions: \n",
        "- Why does BPE help?\n",
        "- Why is byte-level BPE more compact than the char-level BPE?\n",
        "- Can you have unknown tokens if you use WordPiece, SentencePiece, or byte-level BPE?\n",
        "- Imagine you are building a multilingual NLP system (very different languages with different characters), what kind of tokenization method would you use?\n",
        "Byte Level\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZp58xbWe7m-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}